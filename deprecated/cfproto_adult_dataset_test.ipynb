{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cj/76jxqn71757384mssvb297jw0000gn/T/ipykernel_70568/3309035374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_adult_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmin_max_scale_numerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_missing_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_dummy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from time import time\n",
    "from utils.df_loader import load_adult_df\n",
    "from utils.preprocessing import min_max_scale_numerical, remove_missing_values, inverse_dummy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from alibi.explainers import CounterFactualProto, CounterFactual\n",
    "from alibi_cf.utils import get_cat_vars_dict\n",
    "\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "tf.keras.backend.clear_session()\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # False\n",
    "\n",
    "\n",
    "seed = 123\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, feature_names, numerical_cols, categorical_cols, columns_type, target_name, possible_outcomes = load_adult_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df, scaler = min_max_scale_numerical(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(scaled_df, columns=  [ col for col in categorical_cols if col != target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We should have this amount of input features.\n",
    "sum([len(scaled_df[col].unique()) for col in categorical_cols if col != target_name]) + len(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enconded_df, encoder_dict = label_encode(scaled_df, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_ohe_cat = {}\n",
    "for c_col in categorical_cols:\n",
    "    if c_col != target_name:\n",
    "        cat_to_ohe_cat[c_col] = [ ohe_col for ohe_col in dummy_df.columns if ohe_col.startswith(c_col) and ohe_col != target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_feature_names = [ col for col in dummy_df.columns if col != target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dummy(dummy_df, cat_to_ohe_cat).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "target_label_encoder = LabelEncoder()\n",
    "dummy_df[target_name] = target_label_encoder.fit_transform(dummy_df[target_name])\n",
    "\n",
    "dummy_df= dummy_df[ohe_feature_names + [target_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dummy_df, train_size=.8, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_df[ohe_feature_names])\n",
    "y_train = np.array(train_df[target_name])\n",
    "X_test = np.array(test_df[ohe_feature_names])\n",
    "y_test = np.array(test_df[target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train\n",
    "nn = model= tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(24,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(1),\n",
    "                tf.keras.layers.Activation(tf.nn.sigmoid),\n",
    "            ]\n",
    "        )\n",
    "nn.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(X_train, y_train, batch_size=64, epochs=20, shuffle=True)\n",
    "\n",
    "models = {\n",
    "    \"dt\": DecisionTreeClassifier().fit(X_train,y_train),\n",
    "    \"rfc\": RandomForestClassifier().fit(X_train,y_train),\n",
    "    \"nn\": nn,\n",
    "}\n",
    "\n",
    "pickle.dump(models['dt'], open('./saved_models/dt.p', 'wb'))\n",
    "pickle.dump(models['rfc'], open('./saved_models/rfc.p', 'wb'))\n",
    "models['nn'].save('./saved_models/nn.h5',overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load\n",
    "models = {}\n",
    "models['dt'] = pickle.load(open('./saved_models/dt.p', 'rb'))\n",
    "models['rfc'] = pickle.load(open('./saved_models/rfc.p', 'rb'))\n",
    "models['nn'] = tf.keras.models.load_model('./saved_models/nn.h5')\n",
    "\n",
    "## Initialise NN output shape as (None, 1) for tensorflow.v1\n",
    "models['nn'].predict(np.zeros((2, X_train.shape[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = X_test[0, :].reshape(1,-1)\n",
    "\n",
    "dt_pred = models['dt'].predict(example_data)[0]\n",
    "rfc_pred = models['rfc'].predict(example_data)[0]\n",
    "nn_pred = models['nn'].predict(example_data)[0][0]\n",
    "\n",
    "print(f\"DT [{dt_pred}], RFC [{rfc_pred}], NN [{nn_pred}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alibi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counterfactual Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_dict = get_cat_vars_dict(scaled_df, categorical_cols, feature_names, target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_names = [ col for col in categorical_cols if col != target_name ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_idx_info = []\n",
    "\n",
    "for cat_col in cat_feature_names:\n",
    "    num_unique_v = len([ col for col in train_df.columns if col.startswith(f\"{cat_col}_\")])\n",
    "    first_index = min([ list(train_df.columns).index(col) for col in train_df.columns if col.startswith(f\"{cat_col}_\")])\n",
    "    \n",
    "    cat_vars_idx_info.append({\n",
    "        \"col\": cat_col,\n",
    "        \"num_unique_v\": num_unique_v,\n",
    "        \"first_index\": first_index\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_ohe = {}\n",
    "\n",
    "for idx_info in cat_vars_idx_info:\n",
    "    cat_vars_ohe[idx_info['first_index']] = idx_info['num_unique_v']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_cf import AlibiBinaryPredictWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alibi_wrapped = {\n",
    "    'dt': AlibiBinaryPredictWrapper(models['dt']),\n",
    "    'rfc': AlibiBinaryPredictWrapper(models['rfc']),\n",
    "    'nn': AlibiBinaryPredictWrapper(models['nn']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = (np.ones((1, len(feature_names))), np.zeros((1, len(feature_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_p_dict = {}\n",
    "\n",
    "for k in alibi_wrapped.keys():\n",
    "    cf_p_dict[k] = CounterFactualProto(\n",
    "                                alibi_wrapped[k].predict,\n",
    "                                example_data.shape,\n",
    "                                cat_vars=cat_vars_ohe,\n",
    "                                feature_range=feature_range,\n",
    "                                max_iterations=500,\n",
    "                                ohe=True,\n",
    "                                )\n",
    "\n",
    "    cf_p_dict[k].fit(X_train)\n",
    "    \n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances = 5\n",
    "num_cf_per_instance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for k in cf_p_dict.keys():\n",
    "    results[k] = []\n",
    "    print(f\"Finding counterfactual for {k}\")\n",
    "    for idx, instance in enumerate(X_test[0:num_instances]):\n",
    "        print(f\"instance {idx}\")\n",
    "        example = instance.reshape(1, -1)\n",
    "        for num_cf in range(num_cf_per_instance):\n",
    "            print(f\"CF {num_cf}\")\n",
    "            start_t = time()\n",
    "            exp = cf_p_dict[k].explain(example)\n",
    "            end_t = time ()\n",
    "            running_time = end_t - start_t\n",
    "\n",
    "            if k=='nn':\n",
    "                prediction = target_label_encoder.inverse_transform((models[k].predict(example)[0]> 0.5).astype(int))[0]\n",
    "            else:\n",
    "                prediction = target_label_encoder.inverse_transform(models[k].predict(example))[0]\n",
    "\n",
    "            if (not exp.cf is None) and (len(exp.cf) > 0):\n",
    "                print(\"Found CF\")\n",
    "                if k == 'nn':\n",
    "                    cf = inverse_dummy(pd.DataFrame(exp.cf['X'], columns=ohe_feature_names), cat_to_ohe_cat)\n",
    "                    cf.loc[0, target_name] = target_label_encoder.inverse_transform([exp.cf['class']])[0]\n",
    "                else:\n",
    "                    cf = inverse_dummy(pd.DataFrame(exp.cf['X'], columns=ohe_feature_names), cat_to_ohe_cat)\n",
    "                    cf.loc[0, target_name] = target_label_encoder.inverse_transform([exp.cf['class']])[0]\n",
    "            else:\n",
    "                print(\"CF not found\")\n",
    "                cf = None\n",
    "\n",
    "            input_df = inverse_dummy(pd.DataFrame(example, columns=ohe_feature_names), cat_to_ohe_cat)\n",
    "            input_df.loc[0, target_name] = prediction\n",
    "\n",
    "            results[k].append({\n",
    "                \"input\": input_df,\n",
    "                \"cf\": cf,\n",
    "                'exp': exp,\n",
    "                \"running_time\": running_time,\n",
    "                \"ground_truth\": target_label_encoder.inverse_transform([y_test[idx]])[0],\n",
    "                \"prediction\": prediction,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = {}\n",
    "\n",
    "for k in results.keys():\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(len(results[k])):\n",
    "        final_df = pd.DataFrame([{}])\n",
    "\n",
    "        scaled_input_df = results[k][i]['input'].copy(deep=True)\n",
    "        origin_columns = [f\"origin_input_{col}\"  for col in scaled_input_df.columns]\n",
    "        origin_input_df = scaled_input_df.copy(deep=True)\n",
    "        scaled_input_df.columns = [f\"scaled_input_{col}\"  for col in scaled_input_df.columns]\n",
    "\n",
    "        origin_input_df[numerical_cols] = scaler.inverse_transform(origin_input_df[numerical_cols])\n",
    "        origin_input_df.columns = origin_columns\n",
    "\n",
    "        final_df = final_df.join([scaled_input_df, origin_input_df])\n",
    "\n",
    "        if not results[k][i]['cf'] is None:\n",
    "            scaled_cf_df = results[k][i]['cf'].copy(deep=True)\n",
    "            ## Comment this\n",
    "            # scaled_cf_df.loc[0, target_name] = target_label_encoder.inverse_transform([scaled_cf_df.loc[0, target_name]])[0]\n",
    "            origin_cf_columns = [f\"origin_cf_{col}\"  for col in scaled_cf_df.columns]\n",
    "            origin_cf_df = scaled_cf_df.copy(deep=True)\n",
    "            scaled_cf_df.columns = [f\"scaled_cf_{col}\"  for col in scaled_cf_df.columns]\n",
    "\n",
    "            origin_cf_df[numerical_cols] = scaler.inverse_transform(origin_cf_df[numerical_cols])\n",
    "            origin_cf_df.columns = origin_cf_columns\n",
    "\n",
    "            final_df = final_df.join([scaled_cf_df, origin_cf_df])\n",
    "\n",
    "        # final_df = final_df.join([scaled_input_df, origin_input_df, scaled_cf_df, origin_cf_df])\n",
    "        final_df['running_time'] = results[k][i]['running_time']\n",
    "        final_df['Found'] = \"Y\" if not results[k][i]['cf'] is None else \"N\"\n",
    "        final_df['ground_truth'] = results[k][i]['ground_truth'] \n",
    "        final_df['prediction'] = results[k][i]['prediction'] \n",
    "\n",
    "        all_data.append(final_df)\n",
    "\n",
    "    all_df[k] = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_k in all_df.keys():\n",
    "    all_df[df_k].to_csv(f\"./results/proto_adult_{df_k}_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c622353f32ef24c8d83e5c3e334107c074e82d7c3e8ca52c56b9fc900ce33e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf_mac': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
