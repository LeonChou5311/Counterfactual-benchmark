{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from time import time\n",
    "from utils.df_loader import load_adult_df\n",
    "from utils.preprocessing import remove_missing_values\n",
    "from utils.preprocessing import label_encode, min_max_scale_numerical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from alibi.explainers import CounterFactualProto, CounterFactual\n",
    "from alibi_cf.utils import get_cat_vars_dict\n",
    "\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "tf.keras.backend.clear_session()\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # False\n",
    "\n",
    "\n",
    "seed = 123\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TF version:  2.4.0-rc0\n",
      "Eager execution enabled:  False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df, feature_names, numerical_cols, categorical_cols, columns_type, target_name, possible_outcomes = load_adult_df()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "scaled_df, scaler = min_max_scale_numerical(df, numerical_cols)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "enconded_df, encoder_dict = label_encode(scaled_df, categorical_cols)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_df, test_df = train_test_split(enconded_df, train_size=.8, random_state=seed, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X_train = np.array(train_df[feature_names])\n",
    "y_train = np.array(train_df[target_name])\n",
    "X_test = np.array(test_df[feature_names])\n",
    "y_test = np.array(test_df[target_name])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "### Train\n",
    "nn = model= tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(24,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(12,activation='relu'),\n",
    "                tf.keras.layers.Dense(1),\n",
    "                tf.keras.layers.Activation(tf.nn.sigmoid),\n",
    "            ]\n",
    "        )\n",
    "nn.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(X_train, y_train, batch_size=64, epochs=20, shuffle=True)\n",
    "\n",
    "models = {\n",
    "    \"dt\": DecisionTreeClassifier().fit(X_train,y_train),\n",
    "    \"rfc\": RandomForestClassifier().fit(X_train,y_train),\n",
    "    \"nn\": nn,\n",
    "}\n",
    "\n",
    "pickle.dump(models['dt'], open('./saved_models/dt.p', 'wb'))\n",
    "pickle.dump(models['rfc'], open('./saved_models/rfc.p', 'wb'))\n",
    "models['nn'].save('./saved_models/nn.h5',overwrite=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 26048 samples\n",
      "Epoch 1/20\n",
      "26048/26048 [==============================] - 0s 8us/sample - loss: 0.5347 - acc: 0.7599\n",
      "Epoch 2/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.4705 - acc: 0.7722\n",
      "Epoch 3/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.4284 - acc: 0.7876\n",
      "Epoch 4/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.4144 - acc: 0.7935\n",
      "Epoch 5/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.4063 - acc: 0.8000\n",
      "Epoch 6/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.4000 - acc: 0.8046\n",
      "Epoch 7/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3965 - acc: 0.8039\n",
      "Epoch 8/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3925 - acc: 0.8097\n",
      "Epoch 9/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3893 - acc: 0.8112\n",
      "Epoch 10/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3834 - acc: 0.8131\n",
      "Epoch 11/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3799 - acc: 0.8156\n",
      "Epoch 12/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3770 - acc: 0.8165\n",
      "Epoch 13/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3745 - acc: 0.8182\n",
      "Epoch 14/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3753 - acc: 0.8177\n",
      "Epoch 15/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3696 - acc: 0.8221\n",
      "Epoch 16/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3689 - acc: 0.8223\n",
      "Epoch 17/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3687 - acc: 0.8216\n",
      "Epoch 18/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3670 - acc: 0.8217\n",
      "Epoch 19/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3685 - acc: 0.8213\n",
      "Epoch 20/20\n",
      "26048/26048 [==============================] - 0s 5us/sample - loss: 0.3649 - acc: 0.8210\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### Load\n",
    "models = {}\n",
    "models['dt'] = pickle.load(open('./saved_models/dt.p', 'rb'))\n",
    "models['rfc'] = pickle.load(open('./saved_models/rfc.p', 'rb'))\n",
    "models['nn'] = tf.keras.models.load_model('./saved_models/nn.h5')\n",
    "\n",
    "## Initialise NN output shape as (None, 1) for tensorflow.v1\n",
    "models['nn'].predict(np.zeros((2, 12)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jrhs/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.2731029],\n",
       "       [0.2731029]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "example_data = X_test[0, :].reshape(1,-1)\n",
    "\n",
    "dt_pred = models['dt'].predict(example_data)[0]\n",
    "rfc_pred = models['rfc'].predict(example_data)[0]\n",
    "nn_pred = models['nn'].predict(example_data)[0][0]\n",
    "\n",
    "print(f\"DT [{dt_pred}], RFC [{rfc_pred}], NN [{nn_pred}]\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DT [0], RFC [0], NN [0.18025292456150055]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Alibi"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Counterfactual Prototype"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "cat_vars_dict = get_cat_vars_dict(enconded_df, categorical_cols, feature_names, target_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from alibi_cf import AlibiBinaryPredictWrapper"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "alibi_wrapped = {\n",
    "    'dt': AlibiBinaryPredictWrapper(models['dt']),\n",
    "    'rfc': AlibiBinaryPredictWrapper(models['rfc']),\n",
    "    'nn': AlibiBinaryPredictWrapper(models['nn']),\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "feature_range = (np.amax(X_train, 0).reshape(1, -1), np.amin(X_train, 0).reshape(1, -1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cf_p_dict = {}\n",
    "\n",
    "for k in alibi_wrapped.keys():\n",
    "    cf_p_dict[k] = CounterFactualProto(alibi_wrapped[k].predict, example_data.shape,\n",
    "                                use_kdtree=True, theta=10., max_iterations=1000,\n",
    "                                cat_vars=cat_vars_dict,\n",
    "                                feature_range=feature_range,\n",
    "                                ohe=False,\n",
    "                                c_init=1., c_steps=10,\n",
    "                                )\n",
    "\n",
    "    cf_p_dict[k].fit(X_train)\n",
    "    \n",
    "\"\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "num_instances = 20\n",
    "num_cf_per_instance = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "results = {}\n",
    "for k in cf_p_dict.keys():\n",
    "    results[k] = []\n",
    "    print(f\"Finding counterfactual for {k}\")\n",
    "    for idx, instance in enumerate(X_test[0:num_instances]):\n",
    "        print(f\"instance {idx}\")\n",
    "        example = instance.reshape(1, -1)\n",
    "        for num_cf in range(num_cf_per_instance):\n",
    "            print(f\"CF {num_cf}\")\n",
    "            start_t = time()\n",
    "            exp = cf_p_dict[k].explain(example)\n",
    "            end_t = time ()\n",
    "            running_time = end_t - start_t\n",
    "            results[k].append({\n",
    "                \"input\": example,\n",
    "                \"cf\": exp.cf,\n",
    "                \"running_time\": running_time,\n",
    "            })"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finding counterfactual for dt\n",
      "instance 0\n",
      "instance 1\n",
      "instance 2\n",
      "instance 3\n",
      "instance 4\n",
      "Finding counterfactual for rfc\n",
      "instance 0\n",
      "instance 1\n",
      "instance 2\n",
      "instance 3\n",
      "instance 4\n",
      "Finding counterfactual for nn\n",
      "instance 0\n",
      "instance 1\n",
      "instance 2\n",
      "instance 3\n",
      "instance 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "all_df = {}\n",
    "\n",
    "for k in results.keys():\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(len(results[k])):\n",
    "        row = {}\n",
    "\n",
    "        for f, v in zip(feature_names, results[k][i]['input'].flatten().tolist()):\n",
    "            row[f\"scaled_input_{f}\"] = v  \n",
    "\n",
    "        if not results[k][i]['cf'] is None:\n",
    "            for f, v in zip(feature_names, results[k][i]['cf']['X'].flatten().tolist()):\n",
    "                row[f\"scaled_cf_{f}\"] = v \n",
    "\n",
    "            row[f\"Found\"] = \"Y\"\n",
    "\n",
    "        else:\n",
    "            # for f in zip(feature_names):\n",
    "            #     row[f\"scaled_cf_{f}\"] = \"None\"\n",
    "            for f in feature_names:\n",
    "                row[f\"scaled_cf_{f}\"] = float(\"nan\")\n",
    "            row[f\"Found\"] = \"N\"\n",
    "\n",
    "        row['running_time'] = results[k][i]['running_time']\n",
    "\n",
    "        all_data.append(row)\n",
    "\n",
    "    all_df[k] = pd.DataFrame(all_data)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "all_complete_df = {}\n",
    "\n",
    "for df_k in all_df.keys():\n",
    "    temp_df = all_df[df_k].copy(deep=True)\n",
    "\n",
    "    ### Categorical data \n",
    "    for k in encoder_dict.keys():\n",
    "        if k != target_name:\n",
    "\n",
    "            ### Do it for input \n",
    "            temp_df[f'origin_input_{k}'] = encoder_dict[k].inverse_transform(temp_df[f'scaled_input_{k}'].astype(np.int))\n",
    "\n",
    "            ### Do it for cf\n",
    "            if (len(temp_df.loc[temp_df['Found']=='Y'])) > 0:\n",
    "                temp_df.loc[temp_df['Found']=='Y', f'origin_cf_{k}'] = encoder_dict[k].inverse_transform(temp_df[temp_df['Found']=='Y'][f'scaled_cf_{k}'].astype(np.int))\n",
    "            else:\n",
    "                temp_df[f'origin_cf_{k}'] = [float('nan')] * len(temp_df)\n",
    "\n",
    "    ### Numerical data\n",
    "\n",
    "    temp_df[[f\"origin_input_{col}\" for col in numerical_cols]] = scaler.inverse_transform(temp_df[[f\"scaled_input_{col}\" for col in numerical_cols]])\n",
    "\n",
    "    if (len(temp_df.loc[temp_df['Found']=='Y'])) > 0:\n",
    "        temp_df.loc[temp_df['Found']=='Y',[f\"origin_cf_{col}\" for col in numerical_cols]] = scaler.inverse_transform(temp_df[temp_df['Found']=='Y'][[f\"scaled_cf_{col}\" for col in numerical_cols]])\n",
    "    else: \n",
    "        temp_df[[f\"origin_cf_{col}\" for col in numerical_cols]] = np.ones_like(temp_df[[f\"scaled_input_{col}\" for col in numerical_cols]])* float('nan')\n",
    "\n",
    "    all_complete_df[df_k] = temp_df\n",
    "    all_complete_df[df_k].to_csv(f\"{df_k}_result.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical_col_idxs = [feature_names.index(col) for col in numerical_cols]\n",
    "categorical_col_idxs = [feature_names.index(col) for col in categorical_cols if col != target_name]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf_mac': conda)"
  },
  "interpreter": {
   "hash": "5c622353f32ef24c8d83e5c3e334107c074e82d7c3e8ca52c56b9fc900ce33e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}